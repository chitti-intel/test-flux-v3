{"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"labels":{"app":"kube-prometheus-stack","app.kubernetes.io/instance":"r1","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/part-of":"kube-prometheus-stack","app.kubernetes.io/version":"23.2.0","chart":"kube-prometheus-stack-23.2.0","emco/deployment-id":"2259417618975371430-kube-prometheus-stack","heritage":"Helm","release":"r1"},"name":"r1-kube-prometheus-stack-general.rules","namespace":"default"},"spec":{"groups":[{"name":"general.rules","rules":[{"alert":"TargetDown","annotations":{"description":"{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-targetdown","summary":"One or more targets are unreachable."},"expr":"100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job, namespace, service)) \u003e 10","for":"10m","labels":{"severity":"warning"}},{"alert":"Watchdog","annotations":{"description":"This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n","runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-watchdog","summary":"An alert that should always be firing to certify that Alertmanager is working properly."},"expr":"vector(1)","labels":{"severity":"none"}}]}]}}
